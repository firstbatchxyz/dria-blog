README.md:
  hash: 17016fde8633a794dc44a6f9d7bc5673
  summary: Dria is a comprehensive synthetic data infrastructure designed to empower
    AI development by enabling scalable and efficient data generation workflows. Key
    features include a framework for creating and managing synthetic data pipelines,
    a multi-agent network for data synthesis from web and siloed sources, and tools
    for massive parallelization and compute offloading. Dria supports a wide range
    of AI projects, from traditional predictive models to advanced generative and
    large language model (LLM) applications, providing high-quality and diverse datasets.
    It offers flexible, scalable custom pipelines and an extensive built-in toolset
    to enhance computational efficiency. Dria SDK is available under the MIT License.
cookbook/eval.md:
  hash: ca0b55de58cf4ba1899c9329554d9ab8
  summary: This guide explains how to evaluate Retrieval-Augmented Generation (RAG)
    systems using synthetic data. RAG systems enhance AI-powered question-answering
    applications by retrieving and generating relevant content. The evaluation process
    covers multiple parameters such as embedding models, retrieval methods (e.g.,
    BM25, VectorDB), reranking strategies, and answer generation models. By using
    a Python-based RAG pipeline and generating synthetic question-answer pairs, users
    can test and improve their RAG implementations effectively. The process includes
    setting up the environment with necessary dependencies, generating QA and multi-hop
    questions, and using OpenAI's API for evaluating responses. The guide emphasizes
    the importance of evaluation in optimizing RAG systems, offering a practical approach
    for AI developers and researchers.
cookbook/function_calling.md:
  hash: e6f64274c94b8b6babb74469b0f60380
  summary: The content delves into the nuances and techniques of function calling
    in programming, providing practical insights for software engineers. It covers
    key concepts such as best practices for efficient function calling, coding techniques
    to optimize software development, and essential programming tips to enhance code
    performance. This guide serves as a valuable resource for understanding the intricacies
    of function calling, enabling developers to write more effective and robust code.
    Key aspects include optimizing function usage and understanding the impact of
    function design on overall application performance.
cookbook/nemotron_qa.md:
  hash: a2eef57f08cdb835161579ff538f9ec7
  summary: "This guide provides a comprehensive tutorial on implementing Nvidia's\
    \ Preference Data Pipeline using Dria for synthetic data generation, leveraging\
    \ Meta's Llama 3.1 model. The process involves two main steps: synthetic response\
    \ generation, where multiple questions and corresponding responses are generated\
    \ using Llama 3.1, and a reward model, Nemotron-4 340B, for scoring responses\
    \ to enhance further training. The implementation also details the folder structure,\
    \ use of prompts, flow of data through pipeline steps\u2014subtopics, questions,\
    \ and answers\u2014and the setup of a Dria-based pipeline. Keywords include Nvidia,\
    \ synthetic data, Dria, AI pipeline, Llama 3.1, and data generation. The full\
    \ code is provided along with pointers to run the pipeline efficiently."
cookbook/patient_dialogues.md:
  hash: 1a4e858dfc986359623c57c8c95d1398
  summary: The content explores insightful patient dialogues aimed at enhancing understanding
    and improving communication in healthcare settings. It focuses on key components
    such as patient communication, healthcare dialogues, and medical conversations,
    emphasizing the importance of strengthening the doctor-patient relationship. The
    objective is to provide valuable educational resources for healthcare professionals
    seeking to improve their interaction skills, thereby fostering better health outcomes.
    Keywords include patient communication, healthcare education, and doctor-patient
    relationship.
cookbook/preference_data.md:
  hash: 360c8d17795273858b974fe8f5393b91
  summary: Discover the power of synthetic preference data generation with Dria, a
    tool designed to enhance AI model training and analysis. By creating realistic
    preference data, Dria aids in improving the accuracy and efficiency of AI systems.
    Key topics include synthetic data, preference data, AI training, and innovative
    data generation techniques. Explore how Dria can revolutionize the way AI models
    are developed and fine-tuned.
example_run.md:
  hash: c061849a4f1454fd914bf054404fbae1
  summary: This article provides an example of running asynchronous tasks and parallel
    model execution in Python using Dria. It demonstrates how to use the Dria package,
    including setting up a `Dria` client and `MagPie` instance, and employing a `ParallelSingletonExecutor`
    to run tasks concurrently. The script specifies models such as `Model.GPT4O_MINI`,
    `Model.GEMINI_15_FLASH`, and `Model.MIXTRAL_8_7B`, and loads prompts to query
    the capitals of France and Germany, showcasing how parallel execution can optimize
    workflows in machine learning applications. Key concepts include Python, asynchronous
    tasks, Dria, parallel execution, and machine learning.
factory/clair.md:
  hash: af88a536898aa1944ac3134a15f613ba
  summary: 'Clair is a SingletonTemplate task designed to enhance programming education
    by correcting student solutions and offering detailed feedback. It employs machine
    learning to improve student code, providing a clear reasoning process and generating
    a refined version of the submitted solution. Key features include inputs for task
    descriptions and student submissions, and outputs that echo these inputs alongside
    the corrected solution and the model used. This task is valuable for educational
    technology and code review, and it contributes to improved learning outcomes in
    programming. Keywords: Clair, programming education, student correction, machine
    learning, educational technology, code review.'
factory/code_generation.md:
  hash: a6ceef30e99c924314660dd31980b589
  summary: '`GenerateCode` is a task-focused tool in software engineering for generating
    code from instructions using advanced coding models. It works best with coder
    models like `Model.CODER` and `Model.QWEN2_5_CODER_1_5B`. The main objective is
    to provide code generation capabilities across various programming languages by
    inputting a task description and the desired language. The tool outputs the original
    instruction, the specified language, the generated code, and the model used, making
    it a robust solution for developers seeking to automate or assist in coding tasks.
    Key areas include code generation, programming, and using specific coder models.'
factory/complexity_scorer.md:
  hash: 2fb4b67485cb74b48e8e7583a2d54189
  summary: 'ScoreComplexity is a machine learning workflow designed to rank a list
    of instructions based on their complexity. This Singleton task outputs complexity
    scores for various tasks, such as cooking or writing, enabling users to assess
    and compare instructional difficulty levels. Key features include instruction
    ranking, score generation, and complexity assessment, utilizing the "llama3.1:8b-instruct-fp16"
    model. The task''s efficiency is highlighted in references like the ComplexityScorer
    Distilabel and a study on automatic data selection in instruction tuning. Keywords:
    complexity scoring, instruction ranking, machine learning, task assessment.'
factory/csv_extender.md:
  hash: bc93e6c8190bfd0417ad68a969bc3619
  summary: The `CSVExtenderPipeline` is a powerful tool for enhancing CSV file management
    by generating new rows with specified subcategories, aiding in data extension
    and organization. By using this pipeline, users can specify the number of new
    independent values and rows to be generated, effectively increasing the complexity
    and detail of the CSV data. Key features include supporting data workflows by
    adding new subcategories to existing categories, making it an ideal solution for
    data management tasks that require extensive CSV manipulation. Keywords include
    CSV, data management, pipeline, data extension, and subcategories.
factory/evaluate.md:
  hash: cbcac75bcb2bac2d5a621c4cba62be6f
  summary: The documentation outlines the `EvaluatePrediction` task, a Singleton task
    designed to assess the contextual and semantic accuracy of predicted answers compared
    to correct answers. Key concepts include predictive modeling, semantic analysis,
    and contextual understanding, highlighting its application in AI tasks and evaluation
    processes. The process involves inputting a prediction, question, and context
    to receive an evaluation result using models like GPT-4O. An example demonstrates
    the workflow, showcasing how predictions are evaluated for correctness in context,
    making it a vital tool for ensuring semantic accuracy in AI-driven predictions.
factory/evolve_complexity.md:
  hash: dc0b56bbb65e2d346bbaf1b46266efb9
  summary: EvolveComplexity is an AI-driven tool designed to enhance instruction complexity,
    particularly for enriching storytelling. It utilizes advanced AI models to transform
    simple instructions into more intricate and engaging narratives, providing outputs
    that include the evolved instruction, original instruction, and the model used
    for generation. The tool focuses on applied AI, instruction generation, and storytelling
    enhancement, using models like GEMMA2_9B_FP16 and others. Key concepts include
    complexity evolution and the use of large language models to follow complex instructions.
    References and resources are available for further exploration, including links
    to related academic papers and GitHub repositories.
factory/graph_builder.md:
  hash: 37c4416ac4987f96b8d7be16d7985447
  summary: The article showcases the use of Dria's `GenerateGraph` task in Python
    to create a concept graph from a given context, highlighting its applicability
    in graph generation, concept mapping, and understanding AI relationships. It explains
    how concepts like Artificial Intelligence, machine learning, and deep learning
    can be graphically represented, with nodes and edges detailing their interconnections.
    The example provided uses specific models such as `Model.LLAMA_3_1_8B_OR` to demonstrate
    the process, ultimately outputting a JSON-like structure of related concepts.
    Key keywords include graph generation, AI relationships, concept mapping, and
    Python example.
factory/instruction_backtranslation.md:
  hash: d810c212a31ca8c70f1b50e7a22dd71d
  summary: The document discusses "Instruction Backtranslation," a task for evaluating
    text generation based on given instructions, which produces a score (1-5) and
    reasoning for the evaluation. It involves the use of parallel execution across
    models using the `ParallelSingletonExecutor` to run the task with different models.
    The process involves evaluating the output text against a reference instruction
    to determine accuracy and relevance, using a specific model to provide the score
    and reasoning. Keywords include Instruction Backtranslation, AI Evaluation, Text
    Generation, Score Generation, and Parallel Execution. The method supports efficient
    AI assessment, offering concise evaluations of how well text responses meet instructional
    goals.
factory/instruction_evolution.md:
  hash: 5cef6b078f113aec78e98a9bb951d70a
  summary: EvolveInstruct is a workflow tool designed to enhance AI content generation
    by mutating prompts with various strategies such as "FRESH_START," "ADD_CONSTRAINTS,"
    "DEEPEN," "CONCRETIZE," "INCREASE_REASONING," and "SWITCH_TOPIC." It accepts input
    prompts and mutation types to provide a mutated prompt output, refining prompt
    engineering for better AI model performance. The tool is a part of the Dria framework
    and supports model-based tasks with models like GEMMA2_9B_FP16. EvolveInstruct
    is useful for AI mutation, prompt enhancement, and optimizing content generation
    processes. Key concepts include prompt engineering, mutation strategies, and AI
    content enhancement.
factory/iterate_code.md:
  hash: 0c386351d58865ae9a56e59b38eef6b3
  summary: IterateCode is a software engineering tool designed to enhance and improve
    existing code based on specific instructions. It supports multiple programming
    languages and focuses on tasks such as code improvement, error handling, and async
    programming. The tool works as a `Singleton` task, taking inputs like the original
    code, a guiding instruction, and the target programming language. Outputs include
    the iterated code, the original instruction, language used, and the model applied
    for code generation. The example demonstrates using the GEMMA2_9B_FP16 model to
    add error handling in Python code, emphasizing useful features for developers
    aiming to streamline software development processes. Keywords include code improvement,
    software development, programming, and error handling.
factory/list_extender.md:
  hash: 787e440c92a84ad04ae84bf7fb64d873
  summary: The ListExtender Pipeline is a dynamic data generation tool designed to
    enhance the organization of lists by creating new subcategories from existing
    items. Primarily developed for data pipeline and list management, the tool uses
    Python and supports asynchronous programming. Users can specify the number of
    subcategories to be generated, which can include a wide range of topics such as
    history, literature, philosophy, psychology, biology, chemistry, astronomy, economics,
    geography, politics, technology, art, and more. The tool provides an efficient
    solution for expanding data architecture, thereby offering utility across various
    domains like education, business, and research. Core keywords include data generation,
    list management, data pipeline, Python, and async programming.
factory/magpie.md:
  hash: a29cae345a13eda97bd141077849827f
  summary: MagPie is an AI dialogue generation tool that creates structured conversations
    between two personas, ideal for applications in persona modeling and natural language
    processing. This Singleton task enables users to input the personas of an instructor
    and a responder, as well as specify the number of dialogue turns, to generate
    a conversational output. The tool employs machine learning models, such as the
    GEMMA2_9B_FP16, to produce dialogues that can simulate various conversation scenarios.
    MagPie supports AI dialogue generation by using conversational AI and maintaining
    fairness and objectivity through advanced bias mitigation techniques. For developers,
    it provides a simple implementation example using the Dria framework.
factory/multihopqa.md:
  hash: 9e06584b0ef39f616e99053c06394271
  summary: The "MultiHopQuestion" task is a specialized AI-based tool for generating
    multi-hop questions from three input documents, designed for advanced reasoning
    applications. It facilitates the creation of questions that require reasoning
    across multiple documents, enhancing natural language processing tasks like question
    generation and reasoning. By using a workflow that takes three documents and set
    constraints, it generates questions categorized into 1-hop, 2-hop, and 3-hop,
    with answers and details about the generative model used. This approach leverages
    applied AI and improves query complexity through multi-document analysis, providing
    a valuable resource for AI-driven decision-making and educational purposes. Key
    concepts include multi-hop questions, applied AI, reasoning, question generation,
    and natural language processing.
factory/persona.md:
  hash: 06697753728b71fe1bb5ee66e6f5c95e
  summary: 'PersonaPipeline is a powerful tool for generating unique personas with
    detailed backstories, ideal for creative projects involving simulation descriptions.
    This class allows users to create multiple personas by defining random variables
    that align with a given context, such as a "cyberpunk city in the year 2077."
    By specifying the number of samples, users can generate diverse and complex character
    profiles that explore various aspects of storytelling, cyberpunk themes, and AI-driven
    narrative creation. The output includes richly detailed backstories, making PersonaPipeline
    a valuable resource for writers and developers focused on simulation, storytelling,
    and character development. Keywords: personas, simulation, storytelling, cyberpunk,
    AI, narrative creation, character profiles, creative projects.'
factory/qa.md:
  hash: a7493a5778b003ee528de45d1294fde4
  summary: 'The QAPipeline class is designed for generating AI personas and simulating
    question-answer interactions using text chunks and simulation descriptions. It
    caters to applications involving dynamic question generation and answer formulation,
    driven by detailed persona descriptions that guide the tone and style of responses.
    The pipeline is customizable through inputs like simulation scenarios, persona
    traits, sample numbers, and text chunks, yielding a list of question-answer pairs.
    Key objectives include enhancing understanding of AI capabilities in generating
    coherent, context-aware interactions without human-annotated data. Core terms
    include: AI personas, QAPipeline, question-answering, simulation, AI personas,
    iterative training, synthetic preferences, instruction-following, reward modeling,
    and performance metrics. This framework is crucial for advancing AI''s ability
    to interact naturally and accurately with complex data inputs.'
factory/quality_evolution.md:
  hash: 62b8dc8cd496fdc034bb9a4c62fe0d2c
  summary: EvolveQuality is a specialized task that enhances the quality of responses
    to prompts by rewriting them using selected methods such as helpfulness, creativity,
    relevance, deepening, and details. This Singleton task involves providing an original
    prompt, response, and chosen method to produce an evolved response, enhancing
    depth and engagement. It supports models like GEMMA2_9B_FP16 and emphasizes natural
    language processing techniques to improve response improvement and AI-driven content
    generation. Key concepts include response evolution, applied AI, and natural language
    processing, with practical applications in improving the clarity and depth of
    generated content.
factory/search.md:
  hash: 29346fdc9a85ce95768be251b08c81ad
  summary: The "SearchPipeline" class is designed for web-based data retrieval, focusing
    on topic aggregation and summarization. Core components include a "PageAggregator"
    for collecting relevant web pages and a "PageSummarizer" for summarizing these
    pages when enabled. Typically implemented in Python, this class finds utility
    in machine learning and data retrieval systems. The class accepts inputs such
    as the topic and a summarization option. Examples and demonstrations highlight
    its application in topics like "artificial intelligence" to efficiently generate
    summarized outputs. Keywords include SearchPipeline, data retrieval, web search,
    machine learning, and Python.
factory/self_instruct.md:
  hash: 7c7309b505622858b7458244a6e60647
  summary: The document introduces "SelfInstruct," a tool designed to generate user
    queries for AI applications, particularly in task management within professional
    environments. As a `Singleton` task, it takes inputs like the number of instructions,
    criteria for query generation, application description, and context to produce
    a list of user queries and specify the model used, such as "GEMMA2_9B_FP16." The
    process is demonstrated with a Python example using the Dria platform, outlining
    how to execute the task and providing an expected output. Key topics include AI
    queries, automation, task management, and user instructions.
factory/semantic_triplet.md:
  hash: 14f29c6ee8754b6dbf9b77b212c8a5f8
  summary: The document outlines the SemanticTriplet task, which generates JSON objects
    containing three textual units (semantic triplets) with specified similarity scores.
    Key components include input parameters like the type of textual unit (e.g., sentence),
    language, high and low similarity scores, and educational difficulty level. Outputs
    are JSON objects with textual units that meet these specifications. The process
    involves using the DRIA platform and models like GEMINI_15_FLASH to achieve accurate
    semantic triplet generation. Keywords include semantic triplet, JSON generation,
    textual similarity, NLP, and data generation.
factory/simple.md:
  hash: 22f9bda517afc248654768fb484afdc8
  summary: The article provides a guide on using the Simple Singleton task for text
    generation with the GEMMA2_9B_FP16 model in Python. It explains the process of
    generating text from a prompt, detailing the necessary code and library imports,
    and highlights how to use the Dria client for executing tasks. Core keywords include
    text generation, Singleton task, Python, GEMMA2_9B_FP16, and code example. An
    example with expected output demonstrates the functionality, showcasing an essential
    tool for developers interested in leveraging AI for automated text creation.
factory/subtopic.md:
  hash: a3f55cb71aff8b570f271bd64ef3cf13
  summary: The SubTopicPipeline class is designed to generate detailed subtopics recursively
    from a main topic, with a focus on enhancing content depth and improving SEO strategies.
    It allows for specifying the depth of the subtopic tree, providing a structured
    and comprehensive breakdown of any subject. This functionality is particularly
    useful for content generation, leveraging artificial intelligence and deep learning
    methodologies to produce nuanced and informative content hierarchies. Important
    keywords include subtopic generation, artificial intelligence, deep learning,
    and SEO strategies. By using this pipeline, users can create content rich in detail,
    which can ultimately contribute to better search engine visibility and engagement.
factory/text_classification.md:
  hash: 58da386206220d32e77fad5118546c72
  summary: 'The content describes a `TextClassification` task in Python, which generates
    a JSON object providing an example for text classification tasks. It involves
    inputs such as task description, language, clarity, and difficulty to produce
    outputs containing ''input_text,'' ''label,'' and ''misleading_label.'' The process
    includes setting up a singleton task, using models like `GEMMA2_9B_FP16`, and
    executing the task through a workflow using the `Dria` client. The example highlights
    categorizing movie reviews as positive or negative, showcasing machine learning
    and natural language processing (NLP) in classifying text. Key points include
    using Python and JSON for NLP, leveraging machine learning models, and providing
    code for practical implementation. Keywords: Text Classification, Python, JSON,
    NLP, Machine Learning, Dria, Task, Model.'
factory/text_matching.md:
  hash: d92dfca17326ac1c603fc8c742ff94e9
  summary: 'The "TextMatching" workflow is a specialized task designed to generate
    JSON objects for effective text matching, focusing on input and positive document
    pairing using advanced natural language processing models. Key components include
    task description, language specification, and example generation utilizing models
    like `GEMMA2_9B_FP16`. This process is particularly beneficial for tasks like
    sentiment analysis, enabling sophisticated text comparison and understanding.
    The workflow also highlights the use of models such as `LLAMA3_1_8B_FP16` and
    `QWEN2_5_72B_OR`, showcasing the capabilities of current NLP models in improving
    text embeddings and classification. Keywords: Text Matching, JSON Generation,
    NLP Models, Sentiment Analysis, Language Specification, Model Utilization.'
factory/text_retrieval.md:
  hash: 6f536d0a1812c535c8a1f8fe8e87dffe
  summary: The "TextRetrieval" task generates JSON objects containing user queries,
    a positive document, and a hard negative document aimed at improving text retrieval
    tasks. It utilizes parameters such as task description, query type, length, clarity,
    language, and difficulty to create relevant content, facilitated by specific models
    like LLAMA3_1_8B_FP16. The task addresses the creation of informative and navigational
    queries for studies involving topics like climate change impacts, offering insights
    into document selection for retrieval systems. Key points include the generation
    of diverse document types and enrichment of text embedding processes with detailed
    examples, serving tasks in text retrieval and data generation.
factory/validate.md:
  hash: 736c6f0065a3f7bd554c9890e7f62ce9
  summary: The document provides an overview of the `ValidatePrediction` workflow,
    a `Singleton` task designed to assess if a predicted answer is semantically and
    contextually accurate compared to a correct answer. Core elements include inputs
    like the prediction and correct answer, and outputs such as a validation boolean
    and the model used, like `GEMMA2_9B_FP16`. This task supports AI model-driven
    semantic evaluation to ensure data accuracy and prediction validation. An example
    code snippet demonstrates how to use the `ValidatePrediction` task with various
    AI models, highlighting keywords like prediction validation, semantic evaluation,
    AI models, data accuracy, and contextual assessment.
factory/web_multi_choice.md:
  hash: 68ee34a3f8842769ebe9d01477a9d5c0
  summary: 'WebMultiChoice is a powerful tool designed to answer multiple-choice questions
    by leveraging web search and evaluation techniques. It processes inputs like the
    question and possible answer choices, conducts web searches, and scrapes relevant
    content to generate and evaluate notes for determining the best answer. This process
    is exemplified by using medical scenarios, such as determining which fetal cells
    are stimulated by betamethasone to produce surfactant, demonstrating the tool''s
    ability in areas like AI evaluation and medical education. Key features include
    search query generation, web scraping, and content analysis, employing models
    like QWEN2_5_7B_FP16 for effective decision-making. Keywords: WebMultiChoice,
    AI evaluation, web search, multiple-choice questions, surfactant production, tocolytics,
    betamethasone, fetal cells.'
how-to/batches.md:
  hash: 2ac5057d9f6b8a819f99ec06b638d175
  summary: This document explains how to use Batches with the ParallelSingletonExecutor
    for parallel execution of instructions in Python, leveraging a Dria client and
    the Singleton pattern. It focuses on running multiple instructions concurrently
    using Python's asyncio library. The guide provides a code example featuring essential
    components such as the Dria client, Singleton task, and ParallelSingletonExecutor.
    It outlines how to set models and load instructions to execute tasks like determining
    the capitals of various countries. Key terms include parallel execution, Dria
    client, Python asyncio, Batches, and Singleton pattern.
how-to/data_enrichment.md:
  hash: 32e424720520df2ea3fc22a21b72b312
  summary: 'Discover how to effectively enhance your datasets with Dria''s powerful
    data enrichment capabilities. Utilizing Dria, transform and enrich data with new
    fields to create richer data representations for analytics, improving machine
    learning models or supporting other downstream tasks. Key features include schema
    definition through Pydantic models, prompt creation for data transformation, and
    detailed examples illustrating text summarization and sentiment analysis. This
    process enriches customer feedback, enhances data quality, and streamlines workflows,
    gaining actionable insights and improving search and filtering of large text corpora.
    Keywords: data enrichment, Dria, analytics, machine learning, customer feedback,
    sentiment analysis, data transformation.'
how-to/data_generators.md:
  hash: 6ff184e25858491266ec66331e885776
  summary: The "Dataset Generator" is a versatile tool designed to effortlessly generate
    and transform datasets using Dria, with support for both prompt-based and singleton
    workflows. Key features include parallel execution, automatic schema validation,
    support for multiple models, search functionalities, and sequential workflow processing.
    Aimed at enhancing AI automation, data transformation, and workflow processing,
    it utilizes the `Prompt` class for prompt-based generation and provides factory-built
    singletons for more customized data generation workflows. The tool supports various
    model configurations, including single, multiple, and pipelined models, to optimize
    the dataset creation process. Keywords include dataset generation, Dria, AI automation,
    data transformation, and workflow processing.
how-to/dria_datasets.md:
  hash: af8d02eca24800d1ca186d90f7c82345
  summary: 'DriaDataset is a robust data generation framework designed for structured
    data management and persistence, ensuring seamless integration with multiple formats,
    including JSON, CSV, and Hugging Face datasets. It offers features like automatic
    data persistence, multi-stage process tracking, and schema validation to maintain
    data consistency. DriaDataset facilitates flexible initialization, allowing users
    to start with an empty dataset or integrate existing data to augment or provide
    instructions. Key functionalities include structured handling of complex datasets,
    intermediate step preservation, and compatibility with various import/export options.
    Leveraging tools like Pydantic for schema definition, DriaDataset serves as a
    comprehensive solution for efficient data management and generation processes.
    Keywords: data generation, data management, schema validation, Hugging Face, dataset.'
how-to/dria_datasets_exports.md:
  hash: 3131ccc7c5e8f6682c5c8cc5e0648013
  summary: The guide provides insights into exporting data from DriaDataset for use
    with HuggingFace's TRL framework, highlighting formats such as pandas DataFrame,
    JSONL, and JSON. It introduces the `Formatter` class for converting data into
    training-ready formats, supporting types like Standard and Conversational. Key
    export formats for TRL trainers, such as STANDARD_LANGUAGE_MODELING and CONVERSATIONAL_PROMPT_COMPLETION,
    are explained, alongside mapping processes for dialogue data. Core keywords include
    DriaDataset, data export, format types, TRL, HuggingFace, and machine learning.
how-to/formatting.md:
  hash: 7b9ab240eeb6083aa46bdbc5bd37566d
  summary: The Formatter class in Dria is a powerful tool designed to convert datasets
    into training-ready formats, compatible with various trainers and styles. It supports
    standard and conversational format types with subtypes like LANGUAGE_MODELING,
    PROMPT_ONLY, and PREFERENCE, making data transformation seamless. The class enables
    easy mapping of data keys for both standard and conversational training styles,
    aiding in preparing datasets for the HuggingFace TRL framework, which includes
    trainers like BCOTrainer and PPOTrainer. Key features include the ability to handle
    FIELD_MAPPING and CONVERSATION_MAPPING, making it an essential component for efficient
    data generation and formatting in machine learning workflows.
how-to/functions.md:
  hash: a11fc7ad518eb83fdc79d1c16be64e5b
  summary: The document provides a comprehensive guide on using built-in and custom
    functions in Dria Nodes for advanced workflow automation, particularly emphasizing
    automation and API integration. It highlights built-in tools like `Operator.FUNCTION_CALLING`
    for executing functions such as the `Stock` tool. Custom functions, including
    `CustomTool` for defining operations and `HttpRequestTool` for making HTTP requests,
    allow users to extend Dria's native capabilities. The document includes examples
    and code snippets for creating custom tools and incorporating them into workflows,
    demonstrating how to perform specific tasks, such as sum calculations and fetching
    cryptocurrency prices. Keywords include Dria Nodes, workflows, custom functions,
    automation, and API integration.
how-to/models.md:
  hash: 5ebee32762eecda2f4a1302f739d60e1
  summary: Explore the Dria Network's comprehensive list of AI models from top developers,
    focusing on categories such as machine learning, model repositories, and artificial
    intelligence. This curated selection includes advanced models like NousTheta,
    Phi3Medium, Gemma2, and OpenAI's GPT-4 Turbo, providing a range of options for
    AI applications with varying parameters and quantization levels. Key offerings
    are detailed from leading organizations such as Microsoft, Meta, Google, and Alibaba,
    making the Dria Network a hub for state-of-the-art AI usage.
how-to/pipelines.md:
  hash: d0f23693b421f3f66f7283c97970274d
  summary: This document provides a comprehensive guide on creating asynchronous pipelines
    to efficiently generate question-and-answer (QA) pairs and manage multiple inputs
    concurrently. Key features include using the `Pipeline` class and `StepTemplate`
    for defining sequences of workflows executed in a specific order, utilizing Dria
    network for parallel execution of steps, and employing built-in callbacks like
    `scatter`, `broadcast`, and `aggregate` for handling input-output mappings. The
    guide also emphasizes the flexibility of pipelines with custom callbacks and multiple
    input management. Essential keywords include asynchronous processing, workflows,
    QA generation, data processing, and pipelines.
how-to/prompters.md:
  hash: a4b71027a4cea26d71acedce525f5e5d
  summary: This guide explains how to use the `Prompt` class with Pydantic in Python
    for generating structured outputs from language models. It covers the setup of
    a prompt with schema validation using Pydantic models, specifically focusing on
    creating prompts with the `Prompt` class for easy data generation. Key steps include
    importing necessary libraries, defining output schemas with Pydantic, initializing
    a DriaDataset, and utilizing the `DatasetGenerator` for executing prompts and
    generating data. The article uses a practical example of generating tweet datasets
    and highlights the use of models such as GPT-4. Important keywords include Pydantic,
    data generation, language models, prompting, and Python programming.
how-to/selecting_models.md:
  hash: 605b8772d9792fbfcef56fe717fe535f
  summary: The content provides an overview of the Dria Network, a system for selecting
    and assigning tasks to various large language models (LLMs) for optimized execution.
    Key features include the ability to specify desired models using the `Model` enum,
    asynchronous task execution across network nodes, and the option to distribute
    a single task to multiple models for comparative analysis. Users can utilize models
    like `LLAMA3_1_8B_FP16` and others provided by platforms such as OLLAMA and OPENAI.
    This functionality supports effective model selection, task assignment, and handling
    of model availability within the network. Key terms include asynchronous execution,
    model selection, task assignment, Dria Network, and LLMs.
how-to/singletons.md:
  hash: f1e0ba2377c3dd31d6d026600f9e09f3
  summary: "The content provides a comprehensive guide on creating and using Singletons\
    \ in Dria, emphasizing their role in efficient task handling through Pydantic\
    \ models. Singletons are pre-built task templates with a standard structure for\
    \ input validation and output formatting. The guide covers using ready-made Singletons\
    \ like 'Simple' from Dria\u2019s Factory and illustrates the process of writing\
    \ custom Singletons tailored to specific needs. It details the essential components\
    \ of a Singleton, including defining input fields, creating an output schema with\
    \ Pydantic, and implementing workflow and callback methods for task execution\
    \ and result processing. This approach to task automation within AI workflows\
    \ leverages the power of Dria, Pydantic, and task manipulation for optimized performance.\
    \ Key concepts include Singleton design, task automation, AI workflows, Pydantic\
    \ integration, and Dria\u2019s Factory utility."
how-to/structured_outputs.md:
  hash: 47aae0a9505bd8f3ca19d5230e8ac207
  summary: The article introduces the concept of structured outputs, a feature that
    ensures consistent and valid responses from models by adhering to a supplied JSON
    Schema. This approach prevents models from omitting required keys or producing
    invalid enum values. Dria Network supports structured outputs for providers like
    OpenAI, Gemini, and Ollama, specifically for models capable of function calling.
    The implementation involves providing a schema to the `WorkflowBuilder` instance
    within the Dria SDK, enabling streamlined and reliable workflows. Key keywords
    include Structured Outputs, JSON Schema, Dria Network, OpenAI, and Workflows.
how-to/tasks.md:
  hash: 9b889f397d05d4cb616960e9b7d6cdf4
  summary: Discover the functionality of tasks within the Dria network, which serve
    as units of work executed asynchronously by compute nodes. Key features include
    model selection, scalable operations, and efficient result retrieval. Tasks are
    published to the network with specified workflows and models, ensuring flexible
    and distributed execution. With no limit on the number of tasks that can be published,
    the system supports scalable, high-efficiency operations. This article guides
    users on creating, executing, and retrieving results from tasks, emphasizing the
    Dria network's robust asynchronous execution and scalability capabilities.
how-to/workflows.md:
  hash: bf8adec6b63917cf6203b5cf7f119fb2
  summary: The article "Custom Workflows within Dria Network" provides a comprehensive
    guide on creating custom workflows using the `dria_workflows` package within the
    Dria Network, focusing on effective task management and automation. It highlights
    the core components of a workflow, including configuration settings, steps, and
    flow management. The guide explains how workflows can be constructed by defining
    steps that interact with Large Language Models (LLMs) and input/output memory
    operations. Key features such as memory operations, step types, and flow control
    through conditional logic are discussed. The tutorial offers practical examples
    of building a workflow to generate and validate random variables, illustrating
    the use of Python code in setting up the workflow. Keywords include Dria Network,
    LLM, workflow development, automation, and task management.
installation.md:
  hash: 3485658193a4c6741292144a1d39ae1b
  summary: This guide provides a comprehensive walkthrough for installing the Dria
    SDK on Python, highlighting key steps such as setting up a compatible environment
    with Python 3.10 via conda and handling dependencies like coincurve. It addresses
    troubleshooting techniques for common installation issues, including GCC-related
    problems, by advising on tools like Homebrew and xcode-select. The guide emphasizes
    Dria's alpha-stage network, accessible through RPCs, and invites users to contribute
    by running a network node. Key terms include Dria SDK, Python Installation, Software
    Development, and Troubleshooting.
modules/structrag.md:
  hash: d1e49074697e89c03cce50a43c88123b
  summary: 'StructRAG is a cutting-edge retrieval-augmented generation (RAG) framework
    designed to enhance large language models (LLMs) in performing knowledge-intensive
    reasoning tasks. Unlike traditional RAG methods, StructRAG employs cognitive-inspired
    structuring techniques to handle scattered and noisy information, automatically
    identifying the optimal structure for given tasks to improve inference accuracy.
    The framework consists of three main modules: StructRAGSynthesize for structuring
    knowledge units, StructRAGSimulate for generating and testing solutions, and StructRAGJudge
    for evaluating and refining outputs. By restructuring documents and conducting
    inference on organized information, StructRAG achieves state-of-the-art results
    across various complex knowledge tasks. Key terms include StructRAG, RAG, large
    language models, cognitive science, and knowledge reasoning.'
modules/structrag2.md:
  hash: a62511c6fea66f4d333adc731b080b06
  summary: StructRAG is an innovative approach designed to enhance the reasoning capabilities
    of large language models (LLMs) through knowledge restructuring and hybrid information
    integration. By utilizing a hybrid router to determine the format of structured
    data, StructRAG enables LLMs to better handle complex tasks by structuring information
    during inference. The methodology involves leveraging components like StructRAGGraph,
    StructRAGCatalogue, and StructRAGAlgorithm, which are critical in processing and
    evaluating the complexity of tasks such as developing machine learning algorithms
    or writing research papers. This approach aims to significantly improve the efficiency
    and effectiveness of LLM reasoning, making it a notable advancement in applied
    AI and machine learning with a focus on knowledge-intensive tasks. Keywords include
    StructRAG, knowledge restructuring, hybrid information, LLM, and machine learning.
node.md:
  hash: 5b48fcf7bb7d9f178225fb36aa5a105f
  summary: 'Set up a node on the Dria decentralized network for AI collaboration with
    ease. The Dria network, developed by FirstBatch, allows users to participate without
    any wallet activity and with a quick installation process. Key steps involve downloading
    the launcher from the Dria website, running it, and entering your ETH wallet private
    key. You''ll then choose a model to serve, with options to enter API keys for
    more tools. MacOS users may need to bypass security warnings, and there might
    be OS-specific variations. For post-setup, fill out a form for a Discord role
    and engage with the community by starring the GitHub repo and following Dria on
    social media. Important keywords: decentralized network, AI collaboration, node
    setup, Dria network, compute node.'
quickstart.md:
  hash: 008e87ae2469c3d2aefeaec0a2fdd5ad
  summary: The guide provides a step-by-step overview of using Dria SDK for generating
    datasets of tweets leveraging large language models (LLMs), specifically using
    Python. Key steps include creating a dataset, attaching a dataset generator, defining
    input instructions and prompts, and executing the task. The example script uses
    the Dria SDK to generate tweets about specified topics. It highlights the ease
    of integrating the SDK with models like GPT-4O for data generation and mentions
    that network capacity and volume are currently limited. Key terms include Dria
    SDK, dataset generation, LLMs, Python, and quickstart guide.
