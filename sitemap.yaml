README.md:
  hash: 7d34612f42046d64939c10d5daebfa12
  summary: Dria is an advanced synthetic data infrastructure designed to enhance AI
    projects through custom pipelines and decentralized compute power. It offers a
    framework for creating, managing, and orchestrating synthetic data pipelines with
    a multi-agent network capable of synthesizing data from diverse sources. Dria
    empowers AI development by providing tools for massive parallelization, compute
    offloading, flexible and scalable custom pipelines, and an extensive built-in
    toolset. Leveraging decentralization, Dria enables hyper-parallelization, diverse
    model outputs, and access to web-based grounding, incorporating human diversity
    to create high-quality synthetic datasets. Suitable for various AI applications,
    Dria accelerates development in areas like classification, generative models,
    and dialogue generation. Key features include decentralization, data generation,
    scalable infrastructure, and AI empowerment.
cookbook/eval.md:
  hash: d5d84abcdaaaa98ae3060f2c82ce7d78
  summary: 'This guide explores methods for evaluating Retrieval-Augmented Generation
    (RAG) systems using synthetic data, aiming to enhance AI-powered question-answering
    applications. Key topics include choosing an embedding model, retrieval methods
    like BM25 and VectorDB, reranking strategies, and answer generation models. The
    guide outlines a practical approach to setting up a RAG pipeline using the RAGatouille
    library and generating diverse synthetic datasets for evaluation using the `QAPipeline`
    and `MultiHopQuestion` classes. Additionally, it introduces an `Evaluator` class
    for assessing RAG system performance by comparing generated responses with ground
    truth. This comprehensive evaluation can identify areas for improvement, optimizing
    the RAG systems for better performance. Keywords: RAG systems, synthetic data,
    question-answering, AI evaluation, deep learning, retrieval methods, RAGatouille.'
cookbook/function_calling.md:
  hash: 47a6b34c996c78fc7b62a5bb994f8edb
  summary: This content delves into the intricacies of function calling in programming,
    highlighting essential best practices and common patterns that promote efficient
    coding. It covers key concepts such as optimizing function performance, minimizing
    overhead, and ensuring code clarity. By exploring these techniques, software engineers
    can enhance software development processes, improve code efficiency, and implement
    effective coding patterns. Key topics include software engineering, function calling,
    programming best practices, and code efficiency in software development.
cookbook/nemotron_qa.md:
  hash: 8399e9f924c3f778629a0f29d4810c2c
  summary: The article explores the implementation of Nvidia's Preference Data Pipeline
    using Dria, focusing on synthetic preference data generation and question-answer
    workflows. It highlights the process of using Meta's Llama 3.1 405B for generating
    synthetic responses and employing Nemotron-4 340B for scoring these responses.
    The pipeline is structured into steps, including subtopic generation, question
    creation, and answer generation, each implemented using Dria's `StepTemplate`.
    The tutorial also covers setting up a folder structure and writing prompts to
    guide each step. Additionally, it provides a Python code example for running the
    pipeline. Key terms include Nvidia, Dria, synthetic data, Llama 3.1, Nemotron,
    workflow, and AI alignment.
cookbook/patient_dialogues.md:
  hash: 53265d2c8c8e79c43ba5c5f7870ce2f7
  summary: Explore the integration of Applied AI in healthcare through real-life patient
    dialogues, aimed at enhancing healthcare communication and improving patient care
    outcomes. This content delves into the use of AI technologies in analyzing medical
    dialogues to boost patient communication and streamline healthcare services. Key
    areas of focus include patient communication, healthcare optimization, and the
    application of AI in medical interactions, providing valuable insights into improving
    overall patient care and service delivery.
cookbook/preference_data.md:
  hash: ee386d24eb763f9447724f1ba4a89f7c
  summary: Learn how to generate synthetic preference data using Dria to enhance machine
    learning applications. This approach focuses on creating high-quality synthetic
    data to improve preference modeling processes. Key topics include synthetic data
    generation, the Dria tool, and its impact on machine learning, providing a robust
    framework for developing better predictive models. Important keywords are synthetic
    data, data generation, machine learning, Dria, and preference modeling.
example_run.md:
  hash: d9396762a5a6c5eccaf285a0708ae015
  summary: This example demonstrates how to use the Dria client package in Python
    to run parallel queries across multiple AI models. It employs the `Dria` client
    with a `MagPie` singleton and the `ParallelSingletonExecutor` class to execute
    batch processing. The executor is configured with AI models like `GPT4O_MINI`,
    `GEMINI_15_FLASH`, and `MIXTRAL_8_7B`, and it runs predefined prompts such as
    "What is the capital of France?" and "What is the capital of Germany?" asynchronously
    using Python's `asyncio`. This process highlights the integration of AI model
    task automation and parallel execution in software development, focusing on efficiency
    and scalable batch processing. Key topics include AI model utilization, asynchronous
    execution, and parallel processing in Python applications.
factory/clair.md:
  hash: 455645c89f71e9fdc468f984bfe698bd
  summary: Clair is an AI-driven SingletonTemplate task designed to improve student
    coding solutions by providing automated feedback and enhanced reasoning for programming
    tasks. It takes a task description and a student's original code, then outputs
    a corrected solution alongside the reasoning behind the corrections. Clair utilizes
    the GEMMA2_9B_FP16 model for code generation, ensuring high-quality corrections.
    Important aspects include code correction, AI education, and tailored feedback
    for student solutions. The system is demonstrated through an example that illustrates
    its ability to refine a student's approach to calculating factorials. Key references
    include advancements in alignment and contrastive revisions.
factory/code_generation.md:
  hash: e959554fcbea0e323cc859b1465a715d
  summary: GenerateCode is a `SingletonTemplate` task designed for code generation
    based on specific instructions in various programming languages. It performs optimally
    with coder models such as `Model.CODER` and `Model.QWEN2_5_CODER_1_5B`. The task
    takes an instruction and a language as inputs and produces the corresponding code
    as output, along with details of the model used. The documentation provides an
    example of generating Python code using the `QWEN2_5_CODER_1_5B` model for calculating
    the factorial of a number. Key topics include code generation, AI models, and
    programming languages, making it relevant for those interested in automated coding
    through AI.
factory/complexity_scorer.md:
  hash: 3f597fbb2a0ca4a74efc026ef262c847
  summary: The "ScoreComplexity" is a task within an AI framework that ranks a list
    of instructions based on their complexity using the GEMMA2_9B model in an asynchronous
    Python environment. The process involves providing input instructions and receiving
    complexity scores as output, which helps in instruction ranking. Important features
    include async Python, complexity scoring, and AI models such as GEMMA2_9B_FP16.
    The article also provides a code example demonstrating how to evaluate instruction
    complexity and highlights references for understanding automatic data selection
    in AI instruction tuning. Key topics include applied AI, complexity scoring, instruction
    ranking, and the use of the GEMMA2 model.
factory/csv_extender.md:
  hash: 8d85b618f5421f021edcdd1cc7f14142
  summary: 'The `CSVExtenderPipeline` class is designed to enhance and extend CSV
    files by generating new rows from existing data, particularly through adding subcategories
    to existing categories. Core features include the ability to specify the number
    of new independent values and rows to be generated, which facilitates the creation
    of expanded datasets. This pipeline supports asynchronous programming, making
    it efficient for processing large data operations. Ideal for data processing and
    generation tasks, it enables users to significantly expand CSV content easily.
    Keywords: CSV file extension, data generation, subcategories, asynchronous programming,
    data processing pipeline.'
factory/evaluate.md:
  hash: 0fcae1fa779ba33661cc6f9e19f86d06
  summary: The "EvaluatePrediction" workflow is designed to assess the contextual
    and semantic correctness of predicted answers in machine learning using the `EvaluatePrediction`
    Singleton task. This Python-based method compares a prediction against a specific
    question and context, providing an evaluation result and the model used. Key features
    include its application in evaluating predictions for contextual correctness,
    utilizing models like GPT-4O, and parsing results through an easy-to-deploy code
    snippet. Key keywords include evaluation, machine learning, contextual correctness,
    predictions, and model comparison.
factory/evolve_complexity.md:
  hash: 396fa82cafc2f07288a157fd97d3e55b
  summary: EvolveComplexity enhances the complexity of given instructions using the
    advanced GEMMA2 model, making it ideal for creative writing and evolving instructional
    tasks. This `Singleton` task takes an original instruction and outputs a more
    intricate version, utilizing AI for creative enhancement and task automation.
    The example provided demonstrates its use with the `GEMMA2_9B_FP16` model to transform
    a simple instruction about writing a cat story into a detailed narrative. It is
    a key tool for instruction enhancement, complexity evolution, and leveraging AI
    models for sophisticated task workflows. Keywords include instruction enhancement,
    creative writing, AI models, task automation, and complexity evolution.
factory/graph_builder.md:
  hash: 887aad6dc82f9b2641717b3df2642f76
  summary: The "GenerateGraph" task is a `Singleton` workflow designed to create a
    graph of concepts and their relationships from a given context, focusing on AI
    context extraction and visualization. It produces an output in a JSON-like format
    with nodes and edges that represent the extracted ontology of terms. This process
    is particularly relevant for graph generation, AI concepts, data visualization,
    and ontology extraction. An example implementation using the Dria client demonstrates
    how concepts like Artificial Intelligence, machine learning, deep learning, and
    neural networks are connected. Key details include the use of models like GEMMA2_9B_FP16
    for extraction and visualization tasks.
factory/instruction_evolution.md:
  hash: 89469311ee5c57aaf21e494866702f03
  summary: EvolveInstruct is a workflow tool designed to mutate prompts for improved
    AI model outcomes using various mutation types such as "FRESH_START," "ADD_CONSTRAINTS,"
    "DEEPEN," "CONCRETIZE," "INCREASE_REASONING," and "SWITCH_TOPIC." It enhances
    the original prompts to create more precise instructions, facilitating better
    responses from AI models like GEMMA. The tool outputs a mutated prompt alongside
    the original and specifies the model used, optimizing instruction crafting for
    deep learning and AI workflows. Key aspects include prompt mutation, instruction
    enhancement, and its application through high-performance models like GEMMA2_9B_FP16.
factory/iterate_code.md:
  hash: 71e2042d9b2d9cef1e577f4aa6a15f63
  summary: IterateCode is a software engineering tool designed to iteratively improve
    existing Python code by following specific instructions. It enhances code quality
    and readability through error handling and software improvement techniques. As
    a `Singleton` task, it takes inputs such as the original code, an instruction
    for guidance, and the target programming language, producing an improved version
    of the code. The tool utilizes models like `GEMMA2_9B_FP16` and `DEEPSEEK_CODER_6_7B`
    to generate iterated code. With keywords including code iteration, software improvement,
    Python programming, error handling, and code quality, IterateCode aims to optimize
    and refine coding practices.
factory/list_extender.md:
  hash: 9f16643f9f82deac2123a21702e83e63
  summary: The `ListExtenderPipeline` is a class designed to enhance and organize
    existing lists by generating new subcategories based on specified items. This
    pipeline allows for improved item categorization and topic management, offering
    a way to granularize and extend lists efficiently. Key features include its ability
    to specify the number of new subcategories, making it highly customizable for
    various data generation needs. Keywords relevant to this tool include List Extension,
    Data Generation, Item Categorization, Pipeline, and Topic Management. The functionality
    is demonstrated through Python code, showcasing how to build and execute the pipeline,
    ultimately resulting in comprehensive elaborations of topics such as Wildlife,
    Computers, Food, Physics, and more.
factory/magpie.md:
  hash: e44d6d361f1cf19059d12579cad2ff08
  summary: MagPie is an AI-based tool designed for generating dialogues between personas,
    making it suitable for creating engaging conversational simulations. It operates
    as a Singleton task and produces a dialogue output as a list of dictionaries,
    detailing interactions between an instructor persona and a responding persona,
    such as an AI assistant. Users can specify the number of dialogue turns, and the
    tool uses models like GEMMA2_9B_FP16. Key features include AI dialogue generation,
    persona simulation, and conversation management. It addresses important issues
    like bias mitigation and fairness in AI, providing insights into responsible AI
    development.
factory/multihopqa.md:
  hash: 1cb92c54513103b053a8f918c37d84df
  summary: The content describes the `MultiHopQuestion` task, a feature in the Dria
    platform designed for generating multi-hop questions from three input documents.
    This task focuses on creating questions that require reasoning across multiple
    texts to find answers, enhancing natural language processing (NLP) capabilities.
    It utilizes a Singleton workflow and generative models such as GEMINI_15_FLASH
    to formulate questions that range from single-source queries to complex, multi-source
    inquiries. Key points include its application in applied AI, multi-hop question
    generation, and the use of specific NLP models. This function is significant for
    those interested in advanced question generation and applied AI, supporting tasks
    like data analysis and knowledge synthesis by leveraging AI tools for seamless
    text comprehension and interrogation. Keywords include MultiHopQA, Question Generation,
    NLP, Applied AI, and Dria.
factory/persona.md:
  hash: d0b7a328f7e23fc178e3071c59938430
  summary: 'The `PersonaPipeline` class generates unique personas based on a given
    simulation description, enabling backstory and persona creation for AI simulations,
    particularly in genres like cyberpunk. By defining random variables that align
    with the simulation''s context, users can produce multiple personas with tailored
    backstories. The process involves generating a specified number of samples, each
    crafted through a series of questions. The example provided demonstrates creating
    personas set in a futuristic cyberpunk city in 2077, showcasing diverse characters
    with distinct backgrounds and cybernetic enhancements. Key features include user-defined
    inputs for simulation descriptions and sample amounts, enabling deep customization
    and detailed persona creation. Key terms include: Personas, Synthetic Data, Backstory
    Generation, AI Simulation, Cyberpunk.'
factory/qa.md:
  hash: becd4fe2493ca157caedc37a0aa3992a
  summary: The "QAPipeline" is a class designed to generate personas and simulate
    question-answer interactions effectively, using AI-driven pipelines. It processes
    simulation descriptions to create detailed personas with backstories and manages
    Q&A interactions, producing multiple samples of questions and answers based on
    provided text chunks. Core features include the ability to customize the simulation
    context, purpose, and persona tone for the answers, which helps in generating
    rich, context-driven responses. Key terms associated with this pipeline are QAPipeline,
    question-answering, personas, and AI pipelines, with a focus on data simulation.
    This solution is especially useful for generating concise and direct responses
    suitable for research or educational purposes.
factory/quality_evolution.md:
  hash: d1afee3a8fc4f6657e8bc08d9ef60df5
  summary: EvolveQuality is a Singleton task designed to enhance the quality of responses
    to prompts by utilizing various rewriting methods. Key methods include "HELPFULNESS,"
    "RELEVANCE," "DEEPENING," "CREATIVITY," and "DETAILS," each aimed at improving
    different aspects of the response for better clarity and engagement. The process
    involves using models like `GEMMA2_9B_FP16` to generate evolved responses. This
    tool is particularly useful for refining AI-generated content, enhancing response
    quality, and improving prompt interactions. Key terms include response quality,
    AI enhancement, and content improvement.
factory/search.md:
  hash: 0d878f5bbd8f562f2d9d5ec8861ed591
  summary: The "SearchPipeline" class in Python is designed to streamline web data
    retrieval and summarization processes. By utilizing components such as the "PageAggregator"
    for page collection and the "PageSummarizer" for content condensing, it efficiently
    handles topics like "entropy-based sampling," simplifying machine learning tasks.
    The pipeline supports asynchronous execution, leveraging Python's asyncio capabilities
    to enhance performance. With keywords like "SearchPipeline," "Data Retrieval,"
    "Web Summarization," "Python," and "Machine Learning," the tool caters to efficient
    data management, and this guide provides example code for implementing search
    and summarize functionalities using the Dria library.
factory/self_instruct.md:
  hash: 4c2a2780e796d123074a6f9e69c1c63c
  summary: The document introduces SelfInstruct, a singleton task designed for generating
    user queries tailored to specific AI applications and contexts. It allows users
    to specify the number of queries, along with criteria for their generation, application
    descriptions, and relevant contexts. The output includes a list of generated queries
    and the model used, such as "GEMMA2_9B_FP16." The example provided demonstrates
    generating diverse task management queries for a professional work environment
    using the Dria platform. Core keywords include SelfInstruct, AI queries, task
    management, GEMMA2, and user input generation.
factory/semantic_triplet.md:
  hash: c5a56abbe0e29263e948346b1d3708ac
  summary: The document provides an overview of the `SemanticTriplet` task, a Python-based
    tool for generating JSON objects with three textual units, each having specified
    semantic similarity scores. Key features include the ability to define these units
    by type (e.g., sentence or paragraph), language, high and low similarity scores,
    and the educational difficulty level required to understand them. The example
    provided demonstrates using the `GEMMA2_9B_FP16` model to create a semantic triplet
    in English with specific parameters. This tool is valuable for applications in
    natural language processing, text similarity analysis, and JSON generation, offering
    flexibility in the customization of semantic relationships.
factory/simple.md:
  hash: 12921b9b3d570ee539c8744d34bdcb64
  summary: The content provides an overview of the "Simple" task, a Singleton pattern
    implemented in Python for text generation based on prompts. Featuring examples
    and code snippets, it uses AI models like GEMMA2_9B_FP16 to produce text from
    a given input prompt. The task outputs include the generated text and the model
    used, aiming to assist in software engineering applications involving natural
    language processing. Key elements include the use of the Dria library, asynchronous
    execution with asyncio, and the integration of models such as qwen2.5:7b-instruct-fp16.
    Keywords include text generation, Singleton pattern, AI models, Python code, and
    Dria.
factory/subtopic.md:
  hash: 19816eb378850a67de43628d0173f24f
  summary: The SubTopicPipeline class is designed for generating subtopics recursively
    to facilitate deep dives into topics, particularly in the fields of Artificial
    Intelligence (AI) and Machine Learning. The pipeline uses a recursive algorithm
    to create a tree of subtopics with a specified depth, allowing for comprehensive
    exploration of topics. Core concepts include pipelines, recursive algorithms,
    and subtopic generation, with applications in AI, deep learning, and machine learning.
    The code example demonstrates how to use the SubTopicPipeline to generate detailed
    subtopics under "Artificial Intelligence" with a maximum depth of 2, yielding
    a diverse range of subtopics such as AI applications, ethical considerations,
    and future trends. Important keywords include subtopic generation, recursive algorithms,
    AI, machine learning, and deep learning.
factory/text_classification.md:
  hash: 717cd75658bf2401a6d08398d04a567d
  summary: 'This guide covers the use of the GEMMA2 model in Python for generating
    examples in text classification tasks. It explains how to use the `TextClassification`
    task, which outputs a JSON object with ''input_text'', ''label'', and ''misleading_label'',
    by providing task parameters such as task description, language, clarity, and
    difficulty level. The example provided demonstrates classifying movie reviews
    as positive or negative using GEMMA2_9B_FP16. Key keywords: text classification,
    GEMMA, machine learning, AI models, Python.'
factory/text_matching.md:
  hash: b832ce416419750d5551c5f92b40d2d4
  summary: The TextMatching task, using the GEMMA2_9B_FP16 model, focuses on generating
    JSON objects for text matching applications, specifically providing 'input' and
    'positive_document' examples. It operates as a Singleton pattern within natural
    language processing (NLP) tasks such as sentiment analysis. This method involves
    creating detailed examples based on task descriptions and language specifications.
    Utilizing the TextMatching workflow with models like GEMMA2_9B_FP16, users can
    generate text matching samples in JSON format, enhancing NLP capabilities through
    improved text embeddings. Key topics include embeddings, JSON generation, and
    the Singleton pattern in text matching.
factory/text_retrieval.md:
  hash: e84dabeb4f90b6a9aee572bcf9279c2b
  summary: 'The TextRetrieval task generates JSON objects containing a user query,
    a relevant positive document, and a misleading hard negative document for text
    retrieval tasks. Key parameters include task description, query type, query length,
    clarity, document word count, language, and difficulty level. This task aids in
    developing and testing natural language processing systems, focusing on text retrieval
    and query generation for improved information retrieval efficiency. The process
    leverages advanced models like GEMMA2_9B_FP16 and llama3.1:8b-instruct-fp16 to
    produce outputs tailored to specific requirements, supporting research and application
    development in managing and retrieving textual data. Keywords: text retrieval,
    query generation, information retrieval, JSON, document generation, NLP.'
factory/validate.md:
  hash: aa85168dbf2609c8bb10e039f1f3c22f
  summary: The document describes `ValidatePrediction`, a Singleton task designed
    to assess if a predicted answer is contextually and semantically correct compared
    to a given correct answer. Key features include inputs of a prediction and a correct
    answer, and outputs that provide a boolean validation and the model used for evaluation.
    This task employs machine learning models such as `GEMMA2_9B_FP16` to ensure AI
    predictions are accurate. Example code illustrates how to utilize the `ValidatePrediction`
    task within a Python environment, emphasizing its importance in AI predictions,
    validation, and semantic correctness. Keywords include validation, AI predictions,
    semantic correctness, machine learning, and Singleton pattern.
factory/web_multi_choice.md:
  hash: 6cabf4dfb7d35605053ea8bb7dc14819
  summary: 'WebMultiChoice is a `Singleton` task designed to answer multiple-choice
    questions by leveraging web searches and AI evaluations. It uses a step-by-step
    workflow to generate a search query based on the question and choices, select
    a relevant URL, scrape content, generate notes, and evaluate them to determine
    the best answer. The task integrates models such as `QWEN2_5_7B_FP16` to provide
    accurate and reliable responses, making it ideal for situations requiring the
    interpretation of complex medical or scientific inquiries. Key features include
    web search integration, AI evaluation, and detailed reasoning for each answer,
    enhancing the accuracy and relevance of results. Keywords: WebMultiChoice, multiple-choice
    questions, web search, AI evaluation, singleton task, QWEN2_5_7B_FP16.'
how-to/batches.md:
  hash: 0f90c27577e2756ba8ac8a4a53642e25
  summary: This guide explains how to leverage Batches, specifically the ParallelSingletonExecutor
    in Dria, to perform parallel instruction execution for optimizing workflows with
    concurrent processing. The process involves creating a Dria client, a Singleton
    task, and a ParallelSingletonExecutor object to run multiple instructions simultaneously.
    The article highlights key aspects of async programming, parallel execution, and
    Python usage, focusing on how to configure and execute tasks with models like
    Model.QWEN2_5_7B_FP16 and Model.LLAMA3_2_3B. Ideal for users wanting to enhance
    efficiency in executing multiple tasks concurrently.
how-to/functions.md:
  hash: 9ba31c9a8c6a5301b6e7f894fd737dd3
  summary: 'Explore Dria Nodes and enhance your workflow capabilities using various
    built-in and custom functions, such as `CustomTool` and `HttpRequestTool`. Dria
    Nodes allows users to efficiently create dynamic workflows by integrating tools
    like Python-based custom functions and executing HTTP requests directly within
    workflows. Key features include built-in `Operator.FUNCTION_CALLING` for executing
    predefined tools and `Operator.FUNCTION_CALLING_RAW` for incorporating custom
    functions. With Dria, you can easily automate tasks like retrieving current stock
    prices or fetching cryptocurrency prices through the Gemini API. Keywords: Dria
    Nodes, workflows, custom functions, HTTP requests, Python, dynamic workflows.'
how-to/models.md:
  hash: 000f36e1f006616c49eea920d028af29
  summary: The Dria Network offers a comprehensive range of AI models from leading
    technology companies like Meta, Microsoft, Google, Alibaba, OpenAI, and more.
    Key models include Meta's Llama3.1 and Llama3.2 series, Microsoft's Phi3 and Phi3.5,
    Google's Gemma2, Alibaba's Qwen2.5, and OpenAI's GPT-4 Turbo. These models vary
    in size and capabilities, from lightweight models with smaller parameter counts
    to advanced models with large-scale parameters, suitable for various applications
    such as coding, instructing, and text generation. The models are characterized
    by their quantization approaches (q8, q4, fp16), context lengths, and specific
    instruction focuses. This array offers high flexibility and large-scale AI potentially
    for diverse sectors. Keywords include AI models, Meta Llama, Microsoft Phi, Google
    Gemma, Alibaba Qwen, OpenAI GPT-4, quantized models, fp16, text generation, coding
    models, and diverse AI applications.
how-to/pipelines.md:
  hash: bb9b5642a6198f7adf8f618ef150cb22
  summary: The document explains how to create efficient pipelines using asynchronous
    processing to generate complex outputs from multiple workflows. Core concepts
    include the use of pipelines to execute workflows in parallel, with steps interconnected
    in a sequence to produce desired outcomes. An example is provided using Nvidia's
    synthetic preference data generation, illustrating steps like generating topics,
    forming questions, and answering them through interconnected workflows. It also
    discusses implementing pipelines with Python, using the `Pipeline` and `StepTemplate`
    classes to define workflows and utilize callbacks for parallel execution. Key
    terms include pipelines, workflows, asynchronous processing, data generation,
    and QA pairs.
how-to/selecting_models.md:
  hash: 754d847b96b8df7ae7ce3254e8b8880d
  summary: 'The Dria Network facilitates efficient task execution on a network of
    LLMs through a Mixture-of-Agents (MoA) infrastructure, allowing for optimal task
    outcomes via model selection. Users can specify which models, such as `LLAMA3_1_8B_FP16`
    or `Llama3.2-3B`, to employ for tasks, or choose providers like OLLAMA and OPENAI.
    The system supports asynchronous task handling, model availability polling, and
    simultaneous task execution across multiple models, useful for comparative analysis.
    This setup enhances task efficiency and flexibility, crucial for workflows requiring
    diverse model capabilities. Key topics: LLM, model selection, task execution,
    Dria Network, asynchronous tasks, MoA infrastructure.'
how-to/singletons.md:
  hash: 139406cc416a1a6969771c134634c54c
  summary: This guide on Singletons in the Dria SDK explains how to utilize pre-built
    tasks for efficient task execution without custom coding, enhancing code reusability
    and task management. It covers how to use and create singletons, detailing the
    process of importing singleton classes and creating a `Task` instance. Key components
    include the `workflow` and `parse_result` methods, which execute tasks and format
    results, respectively. The guide also describes creating custom singletons by
    inheriting from the `SingletonTemplate` class, ideal for tasks not covered by
    factory singletons. Key topics include Singletons, Dria SDK, custom workflows,
    and task management.
how-to/structured_outputs.md:
  hash: 059ec34f4f85afc5c95427f77a0ed630
  summary: This article explains how to use structured outputs with the Dria SDK for
    generating valid JSON responses, specifically focusing on book review generation.
    It covers using structured JSON Schema to ensure consistent outputs, detailing
    the process of implementing structured outputs through the Dria Network and compatible
    models like GPT4O_MINI. Key steps include defining a schema for the book review,
    creating a SingletonTemplate for workflow generation, and executing the workflow
    with the Dria client. The article is accompanied by comprehensive Python code
    examples to guide users in leveraging structured outputs efficiently. Key topics
    include structured outputs, JSON Schema, Dria SDK, and book review generation.
how-to/tasks.md:
  hash: 6b94d1e1c7d74de065a5a129b2f5321f
  summary: 'The Dria network revolves around tasks, which are units of work executed
    by nodes in a distributed system. Key features include model selection for flexible
    task assignments, asynchronous execution for efficient workload distribution,
    and scalability allowing unlimited task publication. Tasks are executed using
    specified workflows and models, with results retrieved from the network post-execution.
    This setup is ideal for handling complex, distributed computing workflows efficiently.
    Keywords: Dria Network, Task Management, Asynchronous Execution, Distributed Computing,
    Model Selection, Scalability.'
how-to/workflows.md:
  hash: 13800683ef46d7af3435e4f45bc6b371
  summary: The article "Custom Workflows within Dria Network" explores how to create
    custom workflows using the Dria Network to streamline complex tasks by integrating
    Large Language Models (LLMs) and memory operations. It outlines the essential
    components of a workflow, including configuration settings, steps, and execution
    flow, and explains how to leverage the Python-based `dria_workflows` package for
    crafting workflows. By providing a detailed guide on memory operations, flow condition
    setup, and step management, the article emphasizes the ability to automate processes
    and enhance inter-step communication. Key terms include Dria Network, custom workflows,
    LLM, Python SDK, automation, and memory operations.
modules/structrag.md:
  hash: 20888237328c14c44379e148c0792c07
  summary: 'StructRAG is a novel framework designed to enhance large language models
    (LLMs) for knowledge-intensive reasoning tasks by utilizing retrieval-augmented
    generation (RAG) methods with cognitive-inspired techniques. It improves LLMs''
    ability to handle scattered and noisy information by automatically identifying
    optimal document structures, restructuring data accordingly, and enhancing inference
    accuracy. The framework demonstrates state-of-the-art results across various complex
    reasoning tasks. Key components include StructRAGSynthesize, StructRAGSimulate,
    and StructRAGJudge, each contributing to transforming raw data into structured,
    quality-controlled knowledge. Keywords include: StructRAG, retrieval-augmented
    generation, LLMs, cognitive techniques, knowledge reasoning.'
modules/structrag2.md:
  hash: c9ecaa4806daa46ecb153c82ce8acdde
  summary: The document introduces "StructRAG," a framework designed to enhance knowledge
    restructuring for Large Language Models (LLMs) by utilizing hybrid information
    processing and cutting-edge algorithms. Central to its implementation is the Hybrid
    Router, which optimizes structured information formats to improve LLMs' reasoning
    capabilities. Key features include the use of the `dria` library, with components
    like `StructRAGGraph`, `StructRAGCatalogue`, and `StructRAGAlgorithm`, to model
    complex workflows and tasks such as scoring the complexity of various instructions.
    The framework aims to boost knowledge-intensive reasoning through inference-time
    hybrid information structuring, as detailed in the associated research paper on
    arXiv. Keywords associated with this content include StructRAG, hybrid information,
    LLMs, knowledge restructuring, and AI algorithms.
node.md:
  hash: cdb89e0ac138d2b05f7c66490cba7a1e
  summary: The provided content outlines the process of setting up a node on the Dria
    network, a decentralized platform for AI collaboration created by FirstBatch.
    It highlights that the setup is quick and requires minimal effort, with no wallet
    activity needed. Users can get started by downloading the launcher from the Dria
    website, running the launcher, and entering their ETH wallet private key. They
    can optionally input API keys for additional tools. The guide also emphasizes
    the simplicity of the setup process across different operating systems, encourages
    users to engage with the Dria community through Discord and GitHub, and follows
    the official instructions for optimal results. Keywords include "Decentralized
    Network," "AI Collaboration," "Node Setup," and "Dria."
quickstart.md:
  hash: d449ff59eea0e60e66516a5ed52c4669
  summary: 'The Dria SDK installation guide offers a quickstart for setting up the
    SDK, obtaining RPC keys, and troubleshooting installation issues for data generation.
    Compatible with Python 3.10 or higher, users are guided to create a Conda environment
    and install essential packages, including Dria and coincurve. The guide addresses
    obtaining RPC tokens for the Dria Community and Pro Networks, with instructions
    for setting up environment variables. Important notes include the network''s alpha
    status, no-cost access requirement via RPC token, and contributions by running
    a node. Troubleshooting GCC and coincurve installation issues are provided, alongside
    support links. Key terms: Dria SDK, RPC token, installation guide, Python, data
    generation.'
